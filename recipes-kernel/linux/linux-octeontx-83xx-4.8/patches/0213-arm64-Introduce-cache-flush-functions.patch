From ff5a06e27504e3872e74f85209b31ea160ab87ca Mon Sep 17 00:00:00 2001
From: Radha Mohan Chintakuntla <rchintakuntla@cavium.com>
Date: Fri, 4 Aug 2017 10:34:21 -0700
Subject: [PATCH 213/375] arm64: Introduce cache flush functions

This patch adds cache flush functions. The L2 cache function is very
specific to Cavium CN8xxx SoCs.

Signed-off-by: Radha Mohan Chintakuntla <rchintakuntla@cavium.com>
---
 arch/arm64/include/asm/assembler.h  | 12 ++++++
 arch/arm64/include/asm/cacheflush.h |  2 +
 arch/arm64/mm/cache.S               | 73 +++++++++++++++++++++++++++++++++++++
 arch/arm64/mm/flush.c               | 43 ++++++++++++++++++++++
 4 files changed, 130 insertions(+)

diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
index d5025c69ca81..489c65a39860 100644
--- a/arch/arm64/include/asm/assembler.h
+++ b/arch/arm64/include/asm/assembler.h
@@ -41,6 +41,18 @@
 	msr	daifclr, #2
 	.endm
 
+/*
+ * Save/disable and restore interrupts.
+ */
+	.macro	save_and_disable_irqs, olddaif
+	mrs	\olddaif, daif
+	disable_irq
+	.endm
+
+	.macro	restore_irqs, olddaif
+	msr	daif, \olddaif
+	.endm
+
 /*
  * Enable and disable debug exceptions.
  */
diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
index 4082a8d49ae5..3f4f97be18ef 100644
--- a/arch/arm64/include/asm/cacheflush.h
+++ b/arch/arm64/include/asm/cacheflush.h
@@ -70,6 +70,8 @@ extern void flush_icache_range(unsigned long start, unsigned long end);
 extern void __flush_dcache_area(void *addr, size_t len);
 extern void __clean_dcache_area_pou(void *addr, size_t len);
 extern long __flush_cache_user_range(unsigned long start, unsigned long end);
+extern void flush_cache_all(void);
+extern void flush_l2c(void);
 
 static inline void flush_cache_mm(struct mm_struct *mm)
 {
diff --git a/arch/arm64/mm/cache.S b/arch/arm64/mm/cache.S
index 07d7352d7c38..45abd7c94656 100644
--- a/arch/arm64/mm/cache.S
+++ b/arch/arm64/mm/cache.S
@@ -24,6 +24,79 @@
 #include <asm/cpufeature.h>
 #include <asm/alternative.h>
 
+/*
+ *	__flush_dcache_all()
+ *
+ *	Flush the whole D-cache.
+ *
+ *	Corrupted registers: x0-x7, x9-x11
+ */
+__flush_dcache_all:
+	dmb	sy		// ensure ordering with previous memory accesses
+	mrs	x0, clidr_el1		// read clidr
+	and	x3, x0, #0x7000000	// extract loc from clidr
+	lsr	x3, x3, #23		// left align loc bit field
+	cbz	x3, finished		// if loc is 0, then no need to clean
+	mov	x10, #0			// start clean at cache level 0
+loop1:
+	add	x2, x10, x10, lsr #1	// work out 3x current cache level
+	lsr	x1, x0, x2		// extract cache type bits from clidr
+	and	x1, x1, #7	// mask of the bits for current cache only
+	cmp	x1, #2			// see what cache we have at this level
+	b.lt	skip			// skip if no cache, or just i-cache
+	save_and_disable_irqs x9	// make CSSELR and CCSIDR access atomic
+	msr	csselr_el1, x10		// select current cache level in csselr
+	isb				// isb to sych the new cssr&csidr
+	mrs	x1, ccsidr_el1		// read the new ccsidr
+	restore_irqs x9
+	and	x2, x1, #7		// extract the length of the cache lines
+	add	x2, x2, #4		// add 4 (line length offset)
+	mov	x4, #0x3ff
+	and	x4, x4, x1, lsr #3	// find maximum number on the way size
+	clz	w5, w4		// find bit position of way size increment
+	mov	x7, #0x7fff
+	and	x7, x7, x1, lsr #13	// extract max number of the index size
+loop2:
+	mov	x9, x4			// create working copy of max way size
+loop3:
+	lsl	x6, x9, x5
+	orr	x11, x10, x6		// factor way and cache number into x11
+	lsl	x6, x7, x2
+	orr	x11, x11, x6		// factor index number into x11
+	dc	cisw, x11		// clean & invalidate by set/way
+	subs	x9, x9, #1		// decrement the way
+	b.ge	loop3
+	subs	x7, x7, #1		// decrement the index
+	b.ge	loop2
+skip:
+	add	x10, x10, #2		// increment cache number
+	cmp	x3, x10
+	b.gt	loop1
+finished:
+	mov	x10, #0			// swith back to cache level 0
+	msr	csselr_el1, x10		// select current cache level in csselr
+	dsb	sy
+	isb
+	ret
+ENDPROC(__flush_dcache_all)
+
+/*
+ *	flush_cache_all()
+ *
+ *	Flush the entire cache system.  The data cache flush is now achieved
+ *	using atomic clean / invalidates working outwards from L1 cache. This
+ *	is done using Set/Way based cache maintenance instructions. The
+ *	instruction cache can still be invalidated back to the point of
+ *	unification in a single instruction.
+ */
+ENTRY(flush_cache_all)
+	mov	x12, lr
+	bl	__flush_dcache_all
+	mov	x0, #0
+	ic	ialluis				// I+BTB cache invalidate
+	ret	x12
+ENDPROC(flush_cache_all)
+
 /*
  *	flush_icache_range(start,end)
  *
diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
index 43a76b07eb32..bc8211d3885a 100644
--- a/arch/arm64/mm/flush.c
+++ b/arch/arm64/mm/flush.c
@@ -90,7 +90,50 @@ void flush_dcache_page(struct page *page)
 }
 EXPORT_SYMBOL(flush_dcache_page);
 
+/*
+ * Flush Cavium CN8xxx L2C
+ */
+void flush_l2c(void)
+{
+#define SYS_CVMCACHE_WBI_L2_INDEXED "#0,c11,c0,#5"
+#define CVM_CACHE_WBI_L2I(enc) { asm volatile \
+		("sys "SYS_CVMCACHE_WBI_L2_INDEXED", %0" : : "r" (enc)); }
+
+	u32 num_sets, num_ways;
+	u32 set, way, is_rtg;
+	u64 cssidr, enc;
+
+	WARN_ON(preemptible());
+
+	/* Select L2 cache */
+	write_sysreg(0x2, csselr_el1);
+	isb();
+	cssidr = read_sysreg(ccsidr_el1);
+
+	num_sets = CACHE_NUMSETS(cssidr);
+	num_ways = CACHE_ASSOCIATIVITY(cssidr);
+
+	/* Clear remote tags */
+	is_rtg = 1;
+	for (way = 0; way < num_ways; way++) {
+		for (set = 0; set < num_sets; set++) {
+			enc = 128 * (set + num_sets * (way + (is_rtg * 16)));
+			CVM_CACHE_WBI_L2I(enc);
+		}
+	}
+
+	/* Clear local tags */
+	is_rtg = 0;
+	for (way = 0; way < num_ways; way++) {
+		for (set = 0; set < num_sets; set++) {
+			enc = 128 * (set + num_sets * (way + (is_rtg * 16)));
+			CVM_CACHE_WBI_L2I(enc);
+		}
+	}
+}
+EXPORT_SYMBOL(flush_l2c);
 /*
  * Additional functions defined in assembly.
  */
 EXPORT_SYMBOL(flush_icache_range);
+EXPORT_SYMBOL(flush_cache_all);
-- 
2.14.1

