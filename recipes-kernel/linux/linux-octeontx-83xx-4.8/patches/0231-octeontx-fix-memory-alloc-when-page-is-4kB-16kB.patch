From 91b3ce0ded73c5082c595a0892f784092041df7d Mon Sep 17 00:00:00 2001
From: Lukasz Bartosik <lb@semihalf.com>
Date: Mon, 21 Aug 2017 10:39:39 +0200
Subject: [PATCH 231/375] octeontx: fix memory alloc when page is 4kB/16kB

When PKO module initalizes it allocates about 300 MB of memory for
its internal usage. When page size was 4kB or 16kB this allocation
could not be made in one chunk therefore it was split to be
done in multiple chunks.

Signed-off-by: Lukasz Bartosik <lb@semihalf.com>
Reviewed-by: Stanislaw Kardach <kda@semihalf.com>
---
 drivers/net/ethernet/cavium/octeontx-83xx/fpa.h    |  16 ++-
 .../net/ethernet/cavium/octeontx-83xx/fpavf_main.c | 137 ++++++++++++++++-----
 2 files changed, 116 insertions(+), 37 deletions(-)

diff --git a/drivers/net/ethernet/cavium/octeontx-83xx/fpa.h b/drivers/net/ethernet/cavium/octeontx-83xx/fpa.h
index bfd6af0c6429..16ee0d40fc89 100644
--- a/drivers/net/ethernet/cavium/octeontx-83xx/fpa.h
+++ b/drivers/net/ethernet/cavium/octeontx-83xx/fpa.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (C) 2016 Cavium, Inc.
+ * Copyright (C) 2017 Cavium, Inc.
  *
  * This program is free software; you can redistribute it and/or modify it
  * under the terms of version 2 of the GNU General Public License
@@ -195,6 +195,13 @@ struct fpapf_com_s {
 
 extern struct fpapf_com_s fpapf_com;
 
+struct memvec {
+	void			*addr;
+	dma_addr_t		iova;
+	u32			size;
+	bool			in_use;
+};
+
 struct fpavf {
 	struct pci_dev		*pdev;
 	void __iomem		*reg_base;
@@ -207,10 +214,9 @@ struct fpavf {
 	u64			num_buffers;
 	u64			alloc_thold;
 
-	/* VA of pool memory start in contiguous allocation */
-	void			*vhpool_addr;
-	dma_addr_t		vhpool_iova;
-	u64			vhpool_size;
+	/* VA of pool memory */
+	u64                     vhpool_memvec_size;
+	struct memvec           *vhpool_memvec;
 	atomic_t		alloc_count;
 	u32			stack_ln_ptrs;
 	void			*pool_addr;
diff --git a/drivers/net/ethernet/cavium/octeontx-83xx/fpavf_main.c b/drivers/net/ethernet/cavium/octeontx-83xx/fpavf_main.c
index b99f6265abd8..6bc96e53b6fc 100644
--- a/drivers/net/ethernet/cavium/octeontx-83xx/fpavf_main.c
+++ b/drivers/net/ethernet/cavium/octeontx-83xx/fpavf_main.c
@@ -136,30 +136,79 @@ static int fpa_vf_addbuffers(struct fpavf *fpa, u64 num_buffers, u32 buf_len)
 
 static int fpa_vf_addmemory(struct fpavf *fpa, u64 num_buffers, u32 buf_len)
 {
-	dma_addr_t iova;
-
-	fpa->vhpool_size = num_buffers * buf_len;
-	fpa->vhpool_addr = dma_zalloc_coherent(&fpa->pdev->dev,
-			fpa->vhpool_size, &fpa->vhpool_iova, GFP_KERNEL);
-	if (!fpa->vhpool_addr) {
-		dev_err(&fpa->pdev->dev, "failed to allocate vhpool memory\n");
-		dma_free_coherent(&fpa->pdev->dev, fpa->pool_size,
-				  fpa->pool_addr, fpa->pool_iova);
+	dma_addr_t iova,  first_addr = -1, last_addr = 0;
+	u32 buffs_per_chunk, chunk_size;
+	u32 i, j, ret = 0;
+
+	chunk_size = MAX_ORDER_NR_PAGES * PAGE_SIZE;
+	buffs_per_chunk = chunk_size / buf_len;
+	fpa->vhpool_memvec_size = (num_buffers + buffs_per_chunk - 1) /
+				   buffs_per_chunk;
+	if (fpa->vhpool_memvec_size > (PAGE_SIZE / sizeof(struct memvec *))) {
+		dev_err(&fpa->pdev->dev,
+			"unable to allocate memory for pointers\n");
+		goto err_unlock;
+	}
+
+	fpa->vhpool_memvec = (struct memvec *)__get_free_page(GFP_KERNEL |
+							      __GFP_NOWARN);
+	if (!fpa->vhpool_memvec) {
+		dev_err(&fpa->pdev->dev, "failed to allocate page\n");
 		return -ENOMEM;
 	}
+	memset(fpa->vhpool_memvec, 0, PAGE_SIZE);
+
+	for (i = 0; i < fpa->vhpool_memvec_size; i++) {
+		fpa->vhpool_memvec[i].size = chunk_size;
+		fpa->vhpool_memvec[i].addr =
+			dma_zalloc_coherent(&fpa->pdev->dev,
+					    fpa->vhpool_memvec[i].size,
+					    &fpa->vhpool_memvec[i].iova,
+					    GFP_KERNEL);
+		if (!fpa->vhpool_memvec[i].addr) {
+			dev_err(&fpa->pdev->dev,
+				"failed to allocate vhpool memory\n");
+			ret = -ENOMEM;
+			goto err_unlock;
+		}
 
-	fpavf_reg_write(fpa, FPA_VF_VHPOOL_START_ADDR(0), fpa->vhpool_iova);
-	fpavf_reg_write(fpa, FPA_VF_VHPOOL_END_ADDR(0),
-			fpa->vhpool_iova + fpa->vhpool_size - 1);
+		fpa->vhpool_memvec[i].in_use = true;
+		if (fpa->vhpool_memvec[i].iova > last_addr)
+			last_addr = fpa->vhpool_memvec[i].iova;
+		if (fpa->vhpool_memvec[i].iova < first_addr)
+			first_addr = fpa->vhpool_memvec[i].iova;
+	}
 
-	iova = fpa->vhpool_iova;
-	while (num_buffers) {
-		fpa_vf_free(fpa, 0, iova, 0);
-		iova += buf_len;
-		num_buffers--;
+	fpavf_reg_write(fpa, FPA_VF_VHPOOL_START_ADDR(0), first_addr);
+	fpavf_reg_write(fpa, FPA_VF_VHPOOL_END_ADDR(0),
+			last_addr + chunk_size - 1);
+
+	for (i = 0; i < fpa->vhpool_memvec_size && num_buffers > 0; i++) {
+		iova = fpa->vhpool_memvec[i].iova;
+		for (j = 0; j < buffs_per_chunk; j++) {
+			fpa_vf_free(fpa, 0, iova, 0);
+			iova += buf_len;
+			num_buffers--;
+			if (num_buffers == 0)
+				break;
+		}
 	}
 
 	return 0;
+
+err_unlock:
+
+	for (i = 0; i < fpa->vhpool_memvec_size; i++)
+		if (fpa->vhpool_memvec[i].in_use) {
+			dma_free_coherent(&fpa->pdev->dev,
+					  fpa->vhpool_memvec[i].size,
+					  fpa->vhpool_memvec[i].addr,
+					  fpa->vhpool_memvec[i].iova);
+			fpa->vhpool_memvec[i].in_use = false;
+		}
+
+	fpa->vhpool_memvec_size = 0x0;
+	return ret;
 }
 
 static int fpa_vf_setup(struct fpavf *fpa, u64 num_buffers, u32 buf_len,
@@ -238,9 +287,10 @@ static int fpa_vf_teardown(struct fpavf *fpa)
 	union mbox_data resp;
 	struct mbox_hdr hdr;
 	union mbox_data req;
-	u64 avail, iova;
-	u64 *buf;
-	int ret;
+	struct memvec *memvec;
+	u64 av, iova, *buf;
+	int ret, i;
+	bool found;
 
 	if (!fpa)
 		return -ENODEV;
@@ -267,15 +317,25 @@ static int fpa_vf_teardown(struct fpavf *fpa)
 	 * single page. For that case, free each buffer as it's taken out of the
 	 * pool.
 	 */
-	avail = fpavf_reg_read(fpa, FPA_VF_VHPOOL_AVAILABLE(0));
-	while (avail) {
+	av = fpavf_reg_read(fpa, FPA_VF_VHPOOL_AVAILABLE(0));
+	while (av) {
+		found = false;
 		iova = fpa_vf_alloc(fpa, 0);
-		if (iova >= fpa->vhpool_iova &&
-		    iova < fpa->vhpool_iova + fpa->vhpool_size &&
-		    fpa->flags & FPA_VF_FLAG_CONT_MEM) {
-			avail = fpavf_reg_read(fpa, FPA_VF_VHPOOL_AVAILABLE(0));
-			continue;
+		for (i = 0; i < fpa->vhpool_memvec_size; i++) {
+			memvec = &fpa->vhpool_memvec[i];
+			if (iova >= memvec->iova &&
+			    iova < memvec->iova + memvec->size &&
+			    fpa->flags & FPA_VF_FLAG_CONT_MEM) {
+				av = fpavf_reg_read(fpa,
+						    FPA_VF_VHPOOL_AVAILABLE(0));
+				found = true;
+				break;
+			}
 		}
+
+		if (found)
+			continue;
+
 		/* If there is a NAT_ALIGN bug here, it means that we'll get a
 		 * different address from FPA than the beginning of the page.
 		 * Therefore we're aligning the address to page size.
@@ -284,7 +344,7 @@ static int fpa_vf_teardown(struct fpavf *fpa)
 			dev_err(&fpa->pdev->dev,
 				"NULL buffer in pool %d of domain %d\n",
 				fpa->subdomain_id, fpa->domain_id);
-			avail = fpavf_reg_read(fpa, FPA_VF_VHPOOL_AVAILABLE(0));
+			av = fpavf_reg_read(fpa, FPA_VF_VHPOOL_AVAILABLE(0));
 			continue;
 		}
 		iova = PAGE_ALIGN(iova);
@@ -292,12 +352,25 @@ static int fpa_vf_teardown(struct fpavf *fpa)
 				 DMA_BIDIRECTIONAL);
 		buf = phys_to_virt(fpa_vf_iova_to_phys(fpa, iova));
 		free_page((u64)buf);
-		avail = fpavf_reg_read(fpa, FPA_VF_VHPOOL_AVAILABLE(0));
+		av = fpavf_reg_read(fpa, FPA_VF_VHPOOL_AVAILABLE(0));
 	}
+
 	/* If allocation was contiguous, free that region */
-	if (fpa->flags & FPA_VF_FLAG_CONT_MEM)
-		dma_free_coherent(&fpa->pdev->dev, fpa->vhpool_size,
-				  fpa->vhpool_addr, fpa->vhpool_iova);
+	if (fpa->flags & FPA_VF_FLAG_CONT_MEM) {
+		for (i = 0; i < fpa->vhpool_memvec_size; i++) {
+			if (fpa->vhpool_memvec[i].in_use) {
+				dma_free_coherent(&fpa->pdev->dev,
+						  fpa->vhpool_memvec[i].size,
+						  fpa->vhpool_memvec[i].addr,
+						  fpa->vhpool_memvec[i].iova);
+				fpa->vhpool_memvec[i].in_use = false;
+			}
+
+			fpa->vhpool_memvec_size = 0x0;
+			free_page((unsigned long)fpa->vhpool_memvec);
+		}
+	}
+
 	/* Finally free the stack */
 	dma_free_coherent(&fpa->pdev->dev, fpa->pool_size,
 			  fpa->pool_addr, fpa->pool_iova);
-- 
2.14.1

