From 45144ee46d16059138d26f05bb424c4cbe9fe176 Mon Sep 17 00:00:00 2001
From: Alex Belits <alex.belits@cavium.com>
Date: Sat, 23 Sep 2017 07:58:17 -0700
Subject: [PATCH 240/375] arm64: Add support for ASID locking for use with
 firmware-assisted interrupts.

Signed-off-by: Alex Belits <Alex.Belits@cavium.com>
---
 arch/arm64/include/asm/mmu_context.h |  3 ++
 arch/arm64/mm/context.c              | 69 +++++++++++++++++++++++++++++++++++-
 2 files changed, 71 insertions(+), 1 deletion(-)

diff --git a/arch/arm64/include/asm/mmu_context.h b/arch/arm64/include/asm/mmu_context.h
index b1892a0dbcb0..6821ec5068cc 100644
--- a/arch/arm64/include/asm/mmu_context.h
+++ b/arch/arm64/include/asm/mmu_context.h
@@ -204,5 +204,8 @@ switch_mm(struct mm_struct *prev, struct mm_struct *next,
 #define activate_mm(prev,next)	switch_mm(prev, next, NULL)
 
 void verify_cpu_asid_bits(void);
+int lock_context(struct mm_struct *mm, int index);
+int unlock_context_by_index(int index);
+bool unlock_context_by_mm(struct mm_struct *mm);
 
 #endif
diff --git a/arch/arm64/mm/context.c b/arch/arm64/mm/context.c
index efcf1f7ef1e4..b2167a36936f 100644
--- a/arch/arm64/mm/context.c
+++ b/arch/arm64/mm/context.c
@@ -35,6 +35,10 @@ static unsigned long *asid_map;
 
 static DEFINE_PER_CPU(atomic64_t, active_asids);
 static DEFINE_PER_CPU(u64, reserved_asids);
+
+#define LOCKED_ASIDS_COUNT 128
+
+static u64 locked_asids[LOCKED_ASIDS_COUNT];
 static cpumask_t tlb_flush_pending;
 
 #define ASID_MASK		(~GENMASK(asid_bits - 1, 0))
@@ -108,6 +112,13 @@ static void flush_context(unsigned int cpu)
 		per_cpu(reserved_asids, i) = asid;
 	}
 
+	/* Set bits for locked ASIDs. */
+	for (i = 0; i < LOCKED_ASIDS_COUNT; i++) {
+		asid = locked_asids[i];
+		if (asid != 0)
+			__set_bit(asid & ~ASID_MASK, asid_map);
+	}
+
 	/* Queue a TLB invalidate and flush the I-cache if necessary. */
 	cpumask_setall(&tlb_flush_pending);
 
@@ -115,9 +126,57 @@ static void flush_context(unsigned int cpu)
 		__flush_icache_all();
 }
 
+int lock_context(struct mm_struct *mm, int index)
+{
+	unsigned long flags;
+	u64 asid;
+
+	if ((index < 0) || (index >= LOCKED_ASIDS_COUNT))
+		return -1;
+	raw_spin_lock_irqsave(&cpu_asid_lock, flags);
+	asid = atomic64_read(&mm->context.id);
+	locked_asids[index] = asid;
+	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);
+	return 0;
+}
+
+int unlock_context_by_index(int index)
+{
+	unsigned long flags;
+
+	if ((index < 0) || (index >= LOCKED_ASIDS_COUNT))
+		return -1;
+	raw_spin_lock_irqsave(&cpu_asid_lock, flags);
+	locked_asids[index] = 0;
+	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);
+	return 0;
+}
+
+bool unlock_context_by_mm(struct mm_struct *mm)
+{
+	int i;
+	unsigned long flags;
+	bool hit = false;
+	u64 asid;
+
+	raw_spin_lock_irqsave(&cpu_asid_lock, flags);
+	asid = atomic64_read(&mm->context.id);
+
+	for (i = 0; i < LOCKED_ASIDS_COUNT; i++) {
+		if (locked_asids[i] == asid) {
+			hit = true;
+			locked_asids[i] = 0;
+		}
+	}
+
+	raw_spin_unlock_irqrestore(&cpu_asid_lock, flags);
+
+	return hit;
+}
+
 static bool check_update_reserved_asid(u64 asid, u64 newasid)
 {
-	int cpu;
+	int i, cpu;
 	bool hit = false;
 
 	/*
@@ -136,6 +195,14 @@ static bool check_update_reserved_asid(u64 asid, u64 newasid)
 		}
 	}
 
+	/* Same mechanism for locked ASIDs */
+	for (i = 0; i < LOCKED_ASIDS_COUNT; i++) {
+		if (locked_asids[i] == asid) {
+			hit = true;
+			locked_asids[i] = newasid;
+		}
+	}
+
 	return hit;
 }
 
-- 
2.14.1

